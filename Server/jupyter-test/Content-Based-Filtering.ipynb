{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0123c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\multicampus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b31e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS\n",
    "from flask_mysqldb import MySQL\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8a495f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conn():\n",
    "    conn = mysql.connector.connect(\n",
    "        host = \"localhost\",\n",
    "        user = \"root\",\n",
    "        password = \"ttrip104!\",\n",
    "        database = \"coredb\",\n",
    "        port = 3306,\n",
    "    )\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7184d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스에 연결\n",
    "conn = create_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fdd4c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커서 생성\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e1fd3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 쿼리 실행\n",
    "query = \"select * from coredb.article where end_date >= now() and status = 'T';\"\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "89e1c182",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Unread result found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mysql\\connector\\cursor_cext.py:436\u001b[0m, in \u001b[0;36mCMySQLCursor.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cnx:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unread_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warnings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cnx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mysql\\connector\\connection_cext.py:872\u001b[0m, in \u001b[0;36mCMySQLConnection.handle_unread_result\u001b[1;34m(self, prepared)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsume_results()\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m unread_result:\n\u001b[1;32m--> 872\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InternalError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnread result found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mInternalError\u001b[0m: Unread result found"
     ]
    }
   ],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f1aea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_conn()\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)\n",
    "article_list = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2d8d776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(article_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60f2ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article 리스트를 pandas DataFrame으로 변환\n",
    "# (\n",
    "#     240504, \n",
    "#     datetime.datetime(2023, 5, 8, 3, 38, 16), \n",
    "#     datetime.datetime(2023, 5, 8, 3, 38, 16), \n",
    "#     '베를린', \n",
    "#     '안녕하세요! 베를린에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.', \n",
    "#     datetime.datetime(2023, 5, 9, 3, 38, 16), \n",
    "#     '독일', \n",
    "#     datetime.datetime(2023, 5, 8, 3, 38, 16), \n",
    "#     'T', \n",
    "#     '베를린에서 문화체험 여행 동행 구합니다!',\n",
    "#     2\n",
    "# )\n",
    "# id, created_at, updated_at, city, content, end_date, nation, start_date, status, title, author_id\n",
    "\n",
    "article_list = pd.DataFrame(article_list, columns=['id',\n",
    "                                       'created_at', \n",
    "                                       'updated_at', \n",
    "                                       'city', \n",
    "                                       'content', \n",
    "                                       'end_date', \n",
    "                                       'nation', \n",
    "                                       'start_date', \n",
    "                                       'status', \n",
    "                                       'title', \n",
    "                                       'author_id'\n",
    "                                      ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "80633393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article list에서 필요한 컬럼만  남기기\n",
    "article_list = article_list[['id', 'content', 'author_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "741e7549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>안녕하세요! 런던에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>안녕하세요! 베를린에서 자연탐방 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>안녕하세요! 서울에서 미식 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>안녕하세요! 오사카에서 역사 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>안녕하세요! 런던에서 레저스포츠 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10006</td>\n",
       "      <td>안녕하세요! 베를린에서 쇼핑 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10007</td>\n",
       "      <td>안녕하세요! 서울에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10008</td>\n",
       "      <td>안녕하세요! 오사카에서 자연탐방 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10009</td>\n",
       "      <td>안녕하세요! 런던에서 미식 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10010</td>\n",
       "      <td>안녕하세요! 베를린에서 역사 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          content  author_id\n",
       "0  10001   안녕하세요! 런던에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "1  10002  안녕하세요! 베를린에서 자연탐방 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "2  10003     안녕하세요! 서울에서 미식 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "3  10004    안녕하세요! 오사카에서 역사 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "4  10005  안녕하세요! 런던에서 레저스포츠 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "5  10006    안녕하세요! 베를린에서 쇼핑 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "6  10007   안녕하세요! 서울에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "7  10008  안녕하세요! 오사카에서 자연탐방 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "8  10009     안녕하세요! 런던에서 미식 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2\n",
       "9  10010    안녕하세요! 베를린에서 역사 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.          2"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5cd9dd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b278e179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "# 중복 데이터 제거\n",
    "article_list = article_list.drop_duplicates()\n",
    "# 1. 결측값(Na, NaN) 제거\n",
    "article_list = article_list.dropna()\n",
    "# 2. content 열 결측값을 빈 문자열로 채움\n",
    "article_list['content'] = article_list['content'].fillna('')\n",
    "len(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a0b22fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'korean' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkorean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# cosine 유사도 계산\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1369\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m     )\n\u001b[1;32m-> 1369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    593\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    594\u001b[0m \n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 600\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     )\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'korean' instead."
     ]
    }
   ],
   "source": [
    "# 불용어를 한국어로 지정하고 tf-idf 계산\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words = 'korean')\n",
    "tfidf_matrix = tfidf.fit_transform(article_list['content'])\n",
    "\n",
    "# cosine 유사도 계산\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim = pd,Dataframe(cosine_sim, index = article_list.id, columns  = article_list.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 텍스트를 원하는 품사의 단어로 토큰화하고, 불용어를 제거하는 작업\n",
    "from eunjeon import Mecab\n",
    "stopwords = ['라', '공동으로', '형식', '사람', '대로', '사람들', '생각', '좋아', '기준으로', '알겠는가', '동시에', '일', '의', '같은', '말하면', '오자마자', '누구', '하기는한데', '아', '할때', '그렇', '두', '어떠', '되', '아래', '여기', '아이야', '로써', '막론하고', '여', '한', '육', '하나', '이런', '저', '어쩔수', '천', '구', '요만한', '때문에', '시키', '지', '틀림없다', '물론', '알았어', '미치다', '다면', '근거', '하지만', '해', '비길수', '여러분', '왜냐하면', '응', '그리하여', '일곱', '많', '조차', '이러한', '인하여', '인', '기점', '우리들', '까닭으로', '잠깐', '그중에서', '이르다', '입각하여', '불구하고', '오히려', '까닭', '할수있어', '관해서', '아이고', '그러한즉', '윗', '비슷하다', '참', '하고', '아이쿠', '형식으로', '일반적으로', '할줄알다', '이천칠', '여전히', '관계없이', '김에', '되는', '모른다', '만약에', '조차도', '공동', '자기', '하기만', '그렇지만', '정도', '따라서', '보면', '타인', '영', '그러니', '서술', '예를', '하기보다는', '버금', '아무도', '서술한바와같이', '와', '이번', '몰', '그만', '지경', '서', '오로지', '라면', '두번째로', '기타', '정도에', '뿐이다', '뿐만', '다음으로', '전부', '하도다', '랏다', '않는다면', '만이', '이리하여', '뿐만아니라', '남', '전후', '도착하다', '누가', '예하면', '근거하여', '의거하여', '일때', '으로서', '아이', '아이구', '이와', '얼마', '인하', '면', '엉엉', '나', '만약', '대해', '하지', '어찌하여', '아무거나', '어떻게', '부터', '자', '이러', '붕붕', '이천팔', '잠시', '연관', '보다', '된다', '이렇게말하자면', '위에서', '듯하', '종합한것과같이', '놀라다', '기준', '그렇게', '삐걱', '이르', '을', '그럼에도', '한데', '여섯', '으로써', '예컨대', '휘익', '종합', '면서', '그러니까', '결론', '바', '그러면', '에서', '이로', '따름', '할', '려고', '소생', '못하다', '다시', '까닭에', '임에', '까지도', '않으면', '예', '둥둥', '에', '쾅쾅', '차라리', '첫', '하면', '가', '아울러', '구나', '이렇게되면', '아니', '도달', '요만', '만은', '앞에서', '집', '뿐', '으로', '많은', '있', '안된다', '고', '관해서는', '때문', '봐라', '그런', '위하', '도착', '도', '그러', '다른', '쉿', '어', '하는것도', '그러므로', '하구나', '은', '여부', '나머지는', '혼자', '줄은', '앞', '겸사겸사', '한다면', '만', '입각', '총적으로', '반드시', '들', '상대', '한켠으로는', '오', '는다면', '이유만으로', '어찌하든지', '해서', '관계', '이러이러하다', '든지', '놀라', '칠', '견지', '중에서', '그리고', '틀림없', '남들', '않기', '한항목', '았', '보는데서', '게', '어찌됏어', '어째서', '이만큼', '둘', '그치', '아니라면', '알', '관계가', '함으로써', '셋', '이러이러', '견지에서', '않다', '겠', '이것', '때', '로', '흐흐', '막론', '만일', '우에', '다섯', '아래윗', '쿵', '한하다', '자기집', '적', '마치', '보', '그런데', '못', '줄은모른다', '김', '단지', '이쪽', '운운', '하기에', '관련이', '어떻해', '어떻', '없', '하지마라', '그러나', '소인', '으면', '해야', '어느', '우선', '너', '몰랏다', '시각', '거나', '같다', '안', '지경이다', '생각이다', '오호', '정도의', '또한', '연관되다', '일반', '기', '무엇때문에', '결론을', '한마디', '네', '저희', '부류', '근거로', '어떤것들', '따라', '본대로', '들자면', '하지마', '봐', '본', '위', '상대적으로', '동안', '이렇게', '좋', '도달하다', '데', '이때', '아니다', '중', '말', '그리', '참나', '그', '이라면', '넷', '이천육', '도다', '심지어', '는가', '일단', '아무', '하는바', '전자', '되다', '요만큼', '무렵', '아니라', '같', '입장에서', '비길', '무슨', '하기', '것', '한하', '었', '와아', '외', '어떤', '의거', '다음', '령', '하다', '줄', '까지', '해야한다', '일것이다', '의해', '할수있다', '를', '않도록', '말하자면', '번', '어떠한', '따위', '듯하다', '낫', '그치지', '그렇지', '반대로', '다음에', '너희', '자면', '동시', '이럴', '하면서', '이럴정도로', '지만', '과', '마', '이', '중의하나', '몰라도', '미치', '팔', '이렇', '이와같다면', '에게', '입장', '따름이다', '없다', '총적', '아니면', '이렇구나', '같이', '윙윙', '달려', '오직', '그런즉', '여덟', '기에', '휴', '해서는', '수', '그들', '그만이다', '너희들', '시간', '것과', '위해서', '부류의', '무엇', '임', '야', '어쩔', '이르기까지', '그래서', '째', '항목', '결과', '후', '하느니', '아니었다면', '앞의것', '니', '관련', '도록', '위하여', '나머지', '삼', '하는', '외에도', '할수록', '낼', '지말고', '불구', '하든지', '아하', '켠', '있다', '사', '것들', '함', '자신', '않다면', '륙', '어떤것', '어쨋든', '다시말하면', '이유는', '이곳', '이유', '하려고하다', '어찌', '낫다', '자마자', '결과에', '느니', '탕탕', '한다', '기점으로', '첫번째로', '비슷', '반대', '아홉', '쓰여', '이천구', '다', '끼익', '하도록시키다', '는', '않', '어때', '우리', '하여금', '들면', '하', '됏', '편이', '의해되다']\n",
    "# NNG: 일반 명사, NNP: 고유 명사, VV: 동사, VA: 형용사\n",
    "def tokenizer(raw, pos=[\"NNG\", \"NNP\", \"VV\", \"VA\"], stopword=stopwords):\n",
    "    m = Mecab()\n",
    "    return [word for word, tag in m.pos(raw) if len(word) > 1 and tag in pos and word not in stopword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f38554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 각 문서의 텍스트를 토큰화하고, 단어의 빈도에 가중치를 부여하여 문서-단어 행렬을 생성\n",
    "# 한글 토큰화를 위해 tokenizer와 불용어(stopwords)를 사용하며, 1-gram과 2-gram을 고려하고 최대 200,000개의 단어를 포함하도록 설정\n",
    "\n",
    "tf = TfidfVectorizer(\n",
    "    stop_words=stopwords, tokenizer=tokenizer, ngram_range=(1, 2), max_features=200000, sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7cd93822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_article_list():\n",
    "    conn = create_conn()\n",
    "    query = \"SELECT id, content, author_id FROM coredb.article where end_date >= now() and status = 'T';\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    article_list = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    article_list = pd.DataFrame(article_list, columns=['id', 'content', 'author_id'])\n",
    "    article_list = article_list[['id', 'content', 'author_id']]\n",
    "    # 정규 표현식 수행하여  'content' 열에 있는 모든 문자열에서 한글과 공백을 제외한 모든 문자를 제거\n",
    "    article_list['content'] = article_list['content'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") \n",
    "    article_list['content'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "    article_list = article_list.dropna() # Null 값 제거\n",
    "    \n",
    "    return article_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4ee85b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import _pickle as pickle\n",
    "\n",
    "mtx_path = 'C:\\model/model.mtx'\n",
    "tf_path = 'C:\\model/tf.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "78abb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def first_learning(tf, mtx_path):\n",
    "    article_list = query_article_list()\n",
    "    X = tf.fit_transform(article_list[\"content\"])\n",
    "    tf.n_docs = len(article_list)\n",
    "    with open(mtx_path, \"wb\") as fw:\n",
    "        pickle.dump((X, article_list.index, article_list.id), fw)\n",
    "    with open(tf_path, \"wb\") as fw:\n",
    "        pickle.dump(tf, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c275fee9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def recommand(more):\n",
    "    with open(mtx_path, \"rb\") as fr:\n",
    "        X, article_index, article_list_id = pickle.load(fr)\n",
    "    with open(tf_path, \"rb\") as fr:\n",
    "        tf = pickle.load(fr)\n",
    "    example_vector = tf.transform([more])\n",
    "    #  함수는 다차원 배열을 일차원 배열로 전환하여 배열의 모든 요소가 연속된 선형 구조를 갖도록 변환\n",
    "    cos_similar = linear_kernel(example_vector, X).flatten()\n",
    "    sim_rank_idx = cos_similar.argsort()[::-1]\n",
    "    sim_rank_idx = sim_rank_idx[:5]\n",
    "    for i in sim_rank_idx:\n",
    "        print(\"Cosine Similarity:\", cos_similar[i])\n",
    "        print(\"Article ID:\", article_list_id[i])\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "46c3e2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\multicampus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "first_learning(tf, mtx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a1b040c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8514818043714283\n",
      "Article ID: 30000\n",
      "------\n",
      "Cosine Similarity: 0.8514818043714283\n",
      "Article ID: 27534\n",
      "------\n",
      "Cosine Similarity: 0.8514818043714283\n",
      "Article ID: 22278\n",
      "------\n",
      "Cosine Similarity: 0.8514818043714283\n",
      "Article ID: 22290\n",
      "------\n",
      "Cosine Similarity: 0.8514818043714283\n",
      "Article ID: 24684\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "recommand(\"서울 역사 여행 서울\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a3f34402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8763032607737282\n",
      "Article ID: 30000\n",
      "------\n",
      "Cosine Similarity: 0.8763032607737282\n",
      "Article ID: 27534\n",
      "------\n",
      "Cosine Similarity: 0.8763032607737282\n",
      "Article ID: 22278\n",
      "------\n",
      "Cosine Similarity: 0.8763032607737282\n",
      "Article ID: 22290\n",
      "------\n",
      "Cosine Similarity: 0.8763032607737282\n",
      "Article ID: 24684\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "recommand(\"서울 역사 여행\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5383eaae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8726598842674674\n",
      "Article ID: 23712\n",
      "------\n",
      "Cosine Similarity: 0.8726598842674674\n",
      "Article ID: 29256\n",
      "------\n",
      "Cosine Similarity: 0.8726598842674674\n",
      "Article ID: 21438\n",
      "------\n",
      "Cosine Similarity: 0.8726598842674674\n",
      "Article ID: 23640\n",
      "------\n",
      "Cosine Similarity: 0.8726598842674674\n",
      "Article ID: 26682\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "recommand(\"베를린 역사 여행\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ea3f985d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.7012221047633034\n",
      "Article ID: 25671\n",
      "------\n",
      "Cosine Similarity: 0.7012221047633034\n",
      "Article ID: 22593\n",
      "------\n",
      "Cosine Similarity: 0.7012221047633034\n",
      "Article ID: 27969\n",
      "------\n",
      "Cosine Similarity: 0.7012221047633034\n",
      "Article ID: 22605\n",
      "------\n",
      "Cosine Similarity: 0.7012221047633034\n",
      "Article ID: 25419\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "recommand(\"베를린 문화\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff1b5a",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "096f44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_article_list():\n",
    "    conn = create_conn()\n",
    "    query = \"SELECT id, content, author_id FROM coredb.article where end_date >= now() and status = 'T';\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    article_list = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    article_list = pd.DataFrame(article_list, columns=['id', 'content', 'author_id'])\n",
    "    article_list = article_list[['id', 'content', 'author_id']]\n",
    "    # 정규 표현식 수행하여  'content' 열에 있는 모든 문자열에서 한글과 공백을 제외한 모든 문자를 제거\n",
    "    article_list['content'] = article_list['content'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") \n",
    "    article_list['content'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "    article_list = article_list.dropna() # Null 값 제거\n",
    "    \n",
    "    return article_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f3c6579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import _pickle as pickle\n",
    "\n",
    "mtx_path = 'C:\\model/model.mtx'\n",
    "tf_path = 'C:\\model/tf.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0a58af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "article_pd = query_article_list()\n",
    "\n",
    "def first_learning(tf, mtx_path, article_pd = article_pd):\n",
    "    X = tf.fit_transform(article_pd[\"content\"])\n",
    "    tf.n_docs = len(article_pd)\n",
    "    with open(mtx_path, \"wb\") as fw:\n",
    "        pickle.dump(X, fw)\n",
    "    with open(tf_path, \"wb\") as fw:\n",
    "        pickle.dump(tf, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1e6084ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - more : 추가 된 데이터(유사도 검사 대상)\n",
    "# - requester_id : 본인 게시글 추천을 피하기 위한 비교 값\n",
    "# - n_recom : 추천 갯수\n",
    "def recommand(more, requester_id, n_recom, article_pd = article_pd):\n",
    "    with open(mtx_path, \"rb\") as fr:\n",
    "        X = pickle.load(fr)\n",
    "    with open(tf_path, \"rb\") as fr:\n",
    "        tf = pickle.load(fr)\n",
    "    example_vector = tf.transform([more])\n",
    "    #  함수는 다차원 배열을 일차원 배열로 전환하여 배열의 모든 요소가 연속된 선형 구조를 갖도록 변환\n",
    "    cos_similar = linear_kernel(example_vector, X).flatten()\n",
    "    sim_rank_idx = cos_similar.argsort()[::-1]\n",
    "    sim_rank_idx = sim_rank_idx[:100]\n",
    "    \n",
    "    i = 0;\n",
    "    cnt = 0;\n",
    "    result = []\n",
    "    dataSize = len(sim_rank_idx)\n",
    "    while(n_recom > cnt and i < dataSize):\n",
    "        type(sim_rank_idx[i])\n",
    "        print(\"Cosine Similarity :\", cos_similar[sim_rank_idx[i]])\n",
    "        print(\"Article ID :\", article_pd.loc[sim_rank_idx[i]]['id'])\n",
    "        print(\"Article CONTENT :\", article_pd.loc[sim_rank_idx[i]]['content'])\n",
    "        print(\"------\")\n",
    "        if (article_pd.loc[i]['author_id'] != requester_id):\n",
    "            result.append(article_pd.loc[sim_rank_idx[i]]['id'])\n",
    "            cnt += 1\n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9ccb4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_learning(tf, mtx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "51f9cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Cosine Similarity : 0.9872943300088868\n",
      "Article ID : 28941\n",
      "Article CONTENT : 안녕하세요! 서울에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.8940\n",
      "------\n",
      "Cosine Similarity : 0.9872943300088868\n",
      "Article ID : 20715\n",
      "Article CONTENT : 안녕하세요! 서울에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.714\n",
      "------\n",
      "Cosine Similarity : 0.9872943300088868\n",
      "Article ID : 28953\n",
      "Article CONTENT : 안녕하세요! 서울에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.8952\n",
      "------\n",
      "Cosine Similarity : 0.9872943300088868\n",
      "Article ID : 26043\n",
      "Article CONTENT : 안녕하세요! 서울에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.6042\n",
      "------\n",
      "Cosine Similarity : 0.9872943300088868\n",
      "Article ID : 20739\n",
      "Article CONTENT : 안녕하세요! 서울에서 문화체험 여행을 계획 중입니다. 함께 여행할 동행을 찾습니다.738\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[28941, 20715, 28953, 26043, 20739]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommand(\"안녕하세요! 서울에서 문화체험 여행을 계획 중입니다. 함께 여행할\", 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b7546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
